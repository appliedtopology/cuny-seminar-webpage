---
title: Can persistent homology whiten Transformer-based black-box models? A case study on BERT compression.
author: Luis Balderas Ruiz
date: 2024-02-16
category: talk
---
Large Language Models (LLMs) like BERT have gained significant prominence due to their remarkable performance in various natural language processing tasks. However, they come with substantial computational and memory costs. Additionally, they are essentially black-box models, challenging to explain and interpret. In this talk, I will discuss about Optimus BERT Compression and Explainability (OBCE), a methodology to bring explainability to BERT models using persistent homology, aiming to measure the importance of each neuron by studying the topological characteristics of their outputs.

